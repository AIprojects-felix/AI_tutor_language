# AI Tutor Cross-Linguistic Learning System Training Configuration

# Basic configuration
model_name: "microsoft/Phi-4-multimodal-instruct"
output_dir: "./models/crosslinguistic_tutor"
cache_dir: "./models/cache"

# Data configuration
data:
  train_data_path: "./data/processed/train.json"
  eval_data_path: "./data/processed/eval.json"
  test_data_path: "./data/processed/test.json"
  max_length: 1024
  train_split_ratio: 0.8
  eval_split_ratio: 0.1
  test_split_ratio: 0.1

# SFT training configuration
sft:
  num_epochs: 3
  batch_size: 4
  learning_rate: 2e-5
  warmup_steps: 500
  weight_decay: 0.01
  gradient_accumulation_steps: 4
  save_steps: 500
  eval_steps: 500
  logging_steps: 10
  max_grad_norm: 1.0
  
# RLHF training configuration
rlhf:
  num_epochs: 2
  batch_size: 2
  mini_batch_size: 1
  learning_rate: 1e-6
  kl_penalty: 0.1
  reward_model_path: "./models/reward_model"
  ppo_epochs: 4
  value_loss_coef: 0.1
  
# Evaluation configuration
evaluation:
  benchmark_path: "./data/benchmark/crosslinguistic_benchmark.json"
  metrics: ["accuracy", "quality", "efficiency", "coherence"]
  batch_size: 8
  save_results: true
  
# Hardware configuration
hardware:
  use_gpu: true
  gpu_ids: [0]
  fp16: true
  gradient_checkpointing: true
  
# Logging configuration
logging:
  use_wandb: true
  wandb_project: "ai-tutor-crosslinguistic"
  log_level: "INFO"
  save_logs: true
  
# Other configuration
seed: 42
num_workers: 4
use_cache: true